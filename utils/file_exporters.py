"""
–ï–∫—Å–ø–æ—Ä—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ñ–≤ –¥–µ—ñ–¥–µ–Ω—Ç–∏—Ñ—ñ–∫–∞—Ü—ñ—ó –≤ —Ä—ñ–∑–Ω—ñ —Ñ–æ—Ä–º–∞—Ç–∏.

–ê—Ä—Ö—ñ—Ç–µ–∫—Ç—É—Ä–Ω–∞ —Å—Ç—Ä–∞—Ç–µ–≥—ñ—è: Strategy Pattern –¥–ª—è –ø—ñ–¥—Ç—Ä–∏–º–∫–∏ –º–Ω–æ–∂–∏–Ω–∏ —Ñ–æ—Ä–º–∞—Ç—ñ–≤.
Extensibility: –õ–µ–≥–∫–µ –¥–æ–¥–∞–≤–∞–Ω–Ω—è –Ω–æ–≤–∏—Ö —Ñ–æ—Ä–º–∞—Ç—ñ–≤ –µ–∫—Å–ø–æ—Ä—Ç—É (PDF, HTML, etc.)
"""

import logging
import json
from pathlib import Path
from datetime import datetime
from typing import List, Optional
from dataclasses import asdict

from docx import Document
from docx.shared import Pt, RGBColor
from docx.enum.text import WD_PARAGRAPH_ALIGNMENT

from core.analyzer import AnalysisResult
from presidio_analyzer import RecognizerResult

logger = logging.getLogger(__name__)


class ExportFormat:
    """–ö–æ–Ω—Å—Ç–∞–Ω—Ç–∏ –ø—ñ–¥—Ç—Ä–∏–º—É–≤–∞–Ω–∏—Ö —Ñ–æ—Ä–º–∞—Ç—ñ–≤ –µ–∫—Å–ø–æ—Ä—Ç—É."""
    TXT = 'txt'
    DOCX = 'docx'
    JSON = 'json'
    CSV = 'csv'
    MARKDOWN = 'md'


class FileExporter:
    """
    –£–Ω—ñ–≤–µ—Ä—Å–∞–ª—å–Ω–∏–π –µ–∫—Å–ø–æ—Ä—Ç–µ—Ä —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ñ–≤ –∞–Ω–∞–ª—ñ–∑—É.
    
    Design Pattern: Facade + Strategy –¥–ª—è —Ä—ñ–∑–Ω–∏—Ö —Ñ–æ—Ä–º–∞—Ç—ñ–≤.
    Responsibility: –ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü—ñ—è AnalysisResult ‚Üí —Ñ–∞–π–ª–æ–≤—ñ —Ñ–æ—Ä–º–∞—Ç–∏.
    """
    
    @staticmethod
    def export_anonymized_text(
        result: AnalysisResult,
        format: str = ExportFormat.TXT,
        include_metadata: bool = True
    ) -> bytes:
        """
        –ï–∫—Å–ø–æ—Ä—Ç—É—î –∞–Ω–æ–Ω—ñ–º—ñ–∑–æ–≤–∞–Ω–∏–π —Ç–µ–∫—Å—Ç —É –≤–∫–∞–∑–∞–Ω–æ–º—É —Ñ–æ—Ä–º–∞—Ç—ñ.
        
        Args:
            result: –†–µ–∑—É–ª—å—Ç–∞—Ç –∞–Ω–∞–ª—ñ–∑—É
            format: –§–æ—Ä–º–∞—Ç –µ–∫—Å–ø–æ—Ä—Ç—É (txt/docx/md)
            include_metadata: –ß–∏ –¥–æ–¥–∞–≤–∞—Ç–∏ –º–µ—Ç–∞–¥–∞–Ω—ñ –Ω–∞ –ø–æ—á–∞—Ç–æ–∫
            
        Returns:
            –ë–∞–π—Ç–∏ —Ñ–∞–π–ª—É –¥–ª—è –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è
        """
        if format == ExportFormat.TXT:
            return FileExporter._export_txt(result, include_metadata)
        elif format == ExportFormat.DOCX:
            return FileExporter._export_docx(result, include_metadata)
        elif format == ExportFormat.MARKDOWN:
            return FileExporter._export_markdown(result, include_metadata)
        else:
            raise ValueError(f"–ù–µ–ø—ñ–¥—Ç—Ä–∏–º—É–≤–∞–Ω–∏–π —Ñ–æ—Ä–º–∞—Ç: {format}")
    
    @staticmethod
    def export_entities_report(
        result: AnalysisResult,
        format: str = ExportFormat.JSON
    ) -> bytes:
        """
        –ï–∫—Å–ø–æ—Ä—Ç—É—î –∑–≤—ñ—Ç –ø—Ä–æ –∑–Ω–∞–π–¥–µ–Ω—ñ —Å—É—Ç–Ω–æ—Å—Ç—ñ.
        
        Args:
            result: –†–µ–∑—É–ª—å—Ç–∞—Ç –∞–Ω–∞–ª—ñ–∑—É
            format: –§–æ—Ä–º–∞—Ç –µ–∫—Å–ø–æ—Ä—Ç—É (json/csv/txt)
            
        Returns:
            –ë–∞–π—Ç–∏ —Ñ–∞–π–ª—É –¥–ª—è –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è
        """
        if format == ExportFormat.JSON:
            return FileExporter._export_entities_json(result)
        elif format == ExportFormat.CSV:
            return FileExporter._export_entities_csv(result)
        elif format == ExportFormat.TXT:
            return FileExporter._export_entities_txt(result)
        else:
            raise ValueError(f"–ù–µ–ø—ñ–¥—Ç—Ä–∏–º—É–≤–∞–Ω–∏–π —Ñ–æ—Ä–º–∞—Ç: {format}")
    
    @staticmethod
    def export_full_report(
        result: AnalysisResult,
        format: str = ExportFormat.DOCX
    ) -> bytes:
        """
        –ï–∫—Å–ø–æ—Ä—Ç—É—î –ø–æ–≤–Ω–∏–π –∑–≤—ñ—Ç (—Ç–µ–∫—Å—Ç + —Å—É—Ç–Ω–æ—Å—Ç—ñ + —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞).
        
        Args:
            result: –†–µ–∑—É–ª—å—Ç–∞—Ç –∞–Ω–∞–ª—ñ–∑—É
            format: –§–æ—Ä–º–∞—Ç (docx/md/txt)
            
        Returns:
            –ë–∞–π—Ç–∏ —Ñ–∞–π–ª—É –¥–ª—è –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è
        """
        if format == ExportFormat.DOCX:
            return FileExporter._export_full_report_docx(result)
        elif format == ExportFormat.MARKDOWN:
            return FileExporter._export_full_report_md(result)
        else:
            return FileExporter._export_full_report_txt(result)
    
    # ============ TXT EXPORTERS ============
    
    @staticmethod
    def _export_txt(result: AnalysisResult, include_metadata: bool) -> bytes:
        """–ï–∫—Å–ø–æ—Ä—Ç –∞–Ω–æ–Ω—ñ–º—ñ–∑–æ–≤–∞–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç—É –≤ TXT."""
        content_parts = []
        
        if include_metadata:
            metadata = FileExporter._generate_metadata_header(result)
            content_parts.append(metadata)
            content_parts.append("=" * 60)
            content_parts.append("")
        
        content_parts.append(result.anonymized_text)
        
        content = "\n".join(content_parts)
        return content.encode('utf-8')
    
    @staticmethod
    def _export_entities_txt(result: AnalysisResult) -> bytes:
        """–ï–∫—Å–ø–æ—Ä—Ç —Å–ø–∏—Å–∫—É —Å—É—Ç–Ω–æ—Å—Ç–µ–π –≤ TXT."""
        lines = [
            "–ó–í–Ü–¢ –ü–†–û –í–ò–Ø–í–õ–ï–ù–Ü –ü–ï–†–°–û–ù–ê–õ–¨–ù–Ü –î–ê–ù–Ü",
            "=" * 60,
            "",
            f"–ó–∞–≥–∞–ª—å–Ω–∞ –∫—ñ–ª—å–∫—ñ—Å—Ç—å —Å—É—Ç–Ω–æ—Å—Ç–µ–π: {result.entities_count}",
            f"–î–∞—Ç–∞ –∞–Ω–∞–ª—ñ–∑—É: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}",
            "",
            "=" * 60,
            ""
        ]
        
        if result.entities_count == 0:
            lines.append("‚úì –ü–µ—Ä—Å–æ–Ω–∞–ª—å–Ω–∏—Ö –¥–∞–Ω–∏—Ö –Ω–µ –≤–∏—è–≤–ª–µ–Ω–æ")
        else:
            # –ì—Ä—É–ø—É—î–º–æ –∑–∞ —Ç–∏–ø–∞–º–∏
            entities_by_type = {}
            for entity in result.entities:
                if entity.entity_type not in entities_by_type:
                    entities_by_type[entity.entity_type] = []
                entities_by_type[entity.entity_type].append(entity)
            
            for entity_type, entities in sorted(entities_by_type.items()):
                lines.append(f"\nüìå {entity_type} ({len(entities)} –∑–Ω–∞–π–¥–µ–Ω–æ)")
                lines.append("-" * 40)
                
                for idx, entity in enumerate(sorted(entities, key=lambda x: x.start), 1):
                    entity_text = result.original_text[entity.start:entity.end]
                    lines.append(
                        f"{idx}. '{entity_text}' "
                        f"[–ø–æ–∑–∏—Ü—ñ—è {entity.start}-{entity.end}, "
                        f"–≤–ø–µ–≤–Ω–µ–Ω—ñ—Å—Ç—å {entity.score:.0%}]"
                    )
        
        content = "\n".join(lines)
        return content.encode('utf-8')
    
    @staticmethod
    def _export_full_report_txt(result: AnalysisResult) -> bytes:
        """–ü–æ–≤–Ω–∏–π –∑–≤—ñ—Ç —É TXT."""
        lines = [
            "–ü–û–í–ù–ò–ô –ó–í–Ü–¢ –î–ï–Ü–î–ï–ù–¢–ò–§–Ü–ö–ê–¶–Ü–á",
            "=" * 60,
            "",
            FileExporter._generate_metadata_header(result),
            "",
            "=" * 60,
            "",
            "–ê–ù–û–ù–Ü–ú–Ü–ó–û–í–ê–ù–ò–ô –¢–ï–ö–°–¢:",
            "-" * 60,
            result.anonymized_text,
            "",
            "=" * 60,
            ""
        ]
        
        # –î–æ–¥–∞—î–º–æ –∑–≤—ñ—Ç –ø—Ä–æ —Å—É—Ç–Ω–æ—Å—Ç—ñ
        entities_report = FileExporter._export_entities_txt(result).decode('utf-8')
        lines.append(entities_report)
        
        content = "\n".join(lines)
        return content.encode('utf-8')
    
    # ============ JSON EXPORTERS ============
    
    @staticmethod
    def _export_entities_json(result: AnalysisResult) -> bytes:
        """–ï–∫—Å–ø–æ—Ä—Ç —Å—É—Ç–Ω–æ—Å—Ç–µ–π —É JSON (–º–∞—à–∏–Ω–Ω–æ-—á–∏—Ç–∞–±–µ–ª—å–Ω–∏–π —Ñ–æ—Ä–º–∞—Ç)."""
        data = {
            "metadata": {
                "analysis_timestamp": datetime.now().isoformat(),
                "total_entities": result.entities_count,
                "original_text_length": len(result.original_text)
            },
            "entities": [
                {
                    "type": entity.entity_type,
                    "text": result.original_text[entity.start:entity.end],
                    "start": entity.start,
                    "end": entity.end,
                    "confidence": round(entity.score, 3)
                }
                for entity in sorted(result.entities, key=lambda x: x.start)
            ],
            "statistics": FileExporter._calculate_statistics(result)
        }
        
        json_str = json.dumps(data, ensure_ascii=False, indent=2)
        return json_str.encode('utf-8')
    
    # ============ CSV EXPORTERS ============
    
    @staticmethod
    def _export_entities_csv(result: AnalysisResult) -> bytes:
        """–ï–∫—Å–ø–æ—Ä—Ç —Å—É—Ç–Ω–æ—Å—Ç–µ–π —É CSV (–¥–ª—è –∞–Ω–∞–ª—ñ–∑—É –≤ Excel)."""
        import csv
        from io import StringIO
        
        output = StringIO()
        writer = csv.writer(output)
        
        # Header
        writer.writerow([
            "–¢–∏–ø —Å—É—Ç–Ω–æ—Å—Ç—ñ", 
            "–¢–µ–∫—Å—Ç", 
            "–ü–æ—á–∞—Ç–æ–∫", 
            "–ö—ñ–Ω–µ—Ü—å", 
            "–í–ø–µ–≤–Ω–µ–Ω—ñ—Å—Ç—å (%)"
        ])
        
        # –î–∞–Ω—ñ
        for entity in sorted(result.entities, key=lambda x: x.start):
            entity_text = result.original_text[entity.start:entity.end]
            writer.writerow([
                entity.entity_type,
                entity_text,
                entity.start,
                entity.end,
                f"{entity.score * 100:.1f}"
            ])
        
        return output.getvalue().encode('utf-8-sig')  # BOM for Excel
    
    # ============ DOCX EXPORTERS ============
    
    @staticmethod
    def _export_docx(result: AnalysisResult, include_metadata: bool) -> bytes:
        """–ï–∫—Å–ø–æ—Ä—Ç –∞–Ω–æ–Ω—ñ–º—ñ–∑–æ–≤–∞–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç—É –≤ DOCX."""
        doc = Document()
        
        # –ù–∞–ª–∞—à—Ç—É–≤–∞–Ω–Ω—è —Å—Ç–∏–ª—ñ–≤
        style = doc.styles['Normal']
        style.font.name = 'Arial'
        style.font.size = Pt(11)
        
        if include_metadata:
            # –ó–∞–≥–æ–ª–æ–≤–æ–∫
            title = doc.add_heading('–ê–Ω–æ–Ω—ñ–º—ñ–∑–æ–≤–∞–Ω–∏–π –¥–æ–∫—É–º–µ–Ω—Ç', level=1)
            title.alignment = WD_PARAGRAPH_ALIGNMENT.CENTER
            
            # –ú–µ—Ç–∞–¥–∞–Ω—ñ
            metadata_para = doc.add_paragraph()
            metadata_text = FileExporter._generate_metadata_header(result)
            metadata_para.add_run(metadata_text).font.size = Pt(9)
            metadata_para.add_run("\n" + "=" * 60 + "\n\n").font.size = Pt(9)
        
        # –ê–Ω–æ–Ω—ñ–º—ñ–∑–æ–≤–∞–Ω–∏–π —Ç–µ–∫—Å—Ç
        doc.add_paragraph(result.anonymized_text)
        
        # –ó–±–µ—Ä—ñ–≥–∞—î–º–æ –≤ bytes
        from io import BytesIO
        buffer = BytesIO()
        doc.save(buffer)
        buffer.seek(0)
        
        return buffer.getvalue()
    
    @staticmethod
    def _export_full_report_docx(result: AnalysisResult) -> bytes:
        """–ü–æ–≤–Ω–∏–π –∑–≤—ñ—Ç —É DOCX –∑ —Ñ–æ—Ä–º–∞—Ç—É–≤–∞–Ω–Ω—è–º."""
        doc = Document()
        
        # Title
        title = doc.add_heading('–ó–≤—ñ—Ç –ø—Ä–æ –¥–µ—ñ–¥–µ–Ω—Ç–∏—Ñ—ñ–∫–∞—Ü—ñ—é', level=1)
        title.alignment = WD_PARAGRAPH_ALIGNMENT.CENTER
        
        # Metadata Section
        doc.add_heading('–ú–µ—Ç–∞–¥–∞–Ω—ñ –∞–Ω–∞–ª—ñ–∑—É', level=2)
        metadata_para = doc.add_paragraph(
            FileExporter._generate_metadata_header(result)
        )
        metadata_para.style.font.size = Pt(10)
        
        # Statistics
        doc.add_heading('–°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞', level=2)
        stats = FileExporter._calculate_statistics(result)
        stats_table = doc.add_table(rows=len(stats) + 1, cols=2)
        stats_table.style = 'Light Grid Accent 1'
        
        # Header row
        stats_table.rows[0].cells[0].text = "–ü–æ–∫–∞–∑–Ω–∏–∫"
        stats_table.rows[0].cells[1].text = "–ó–Ω–∞—á–µ–Ω–Ω—è"
        
        # Data rows
        for idx, (key, value) in enumerate(stats.items(), 1):
            stats_table.rows[idx].cells[0].text = key
            stats_table.rows[idx].cells[1].text = str(value)
        
        doc.add_paragraph()  # Spacer
        
        # Anonymized Text
        doc.add_heading('–ê–Ω–æ–Ω—ñ–º—ñ–∑–æ–≤–∞–Ω–∏–π —Ç–µ–∫—Å—Ç', level=2)
        doc.add_paragraph(result.anonymized_text)
        
        doc.add_page_break()
        
        # Entities Report
        doc.add_heading('–í–∏—è–≤–ª–µ–Ω—ñ —Å—É—Ç–Ω–æ—Å—Ç—ñ', level=2)
        
        if result.entities_count == 0:
            doc.add_paragraph("‚úì –ü–µ—Ä—Å–æ–Ω–∞–ª—å–Ω–∏—Ö –¥–∞–Ω–∏—Ö –Ω–µ –≤–∏—è–≤–ª–µ–Ω–æ")
        else:
            # –ì—Ä—É–ø—É—î–º–æ –∑–∞ —Ç–∏–ø–∞–º–∏
            entities_by_type = {}
            for entity in result.entities:
                if entity.entity_type not in entities_by_type:
                    entities_by_type[entity.entity_type] = []
                entities_by_type[entity.entity_type].append(entity)
            
            for entity_type, entities in sorted(entities_by_type.items()):
                doc.add_heading(f'{entity_type} ({len(entities)})', level=3)
                
                for idx, entity in enumerate(sorted(entities, key=lambda x: x.start), 1):
                    entity_text = result.original_text[entity.start:entity.end]
                    para = doc.add_paragraph(style='List Number')
                    para.add_run(f"'{entity_text}' ").bold = True
                    para.add_run(
                        f"[–ø–æ–∑–∏—Ü—ñ—è {entity.start}-{entity.end}, "
                        f"–≤–ø–µ–≤–Ω–µ–Ω—ñ—Å—Ç—å {entity.score:.0%}]"
                    ).font.size = Pt(9)
        
        # Save to bytes
        from io import BytesIO
        buffer = BytesIO()
        doc.save(buffer)
        buffer.seek(0)
        
        return buffer.getvalue()
    
    # ============ MARKDOWN EXPORTERS ============
    
    @staticmethod
    def _export_markdown(result: AnalysisResult, include_metadata: bool) -> bytes:
        """–ï–∫—Å–ø–æ—Ä—Ç –∞–Ω–æ–Ω—ñ–º—ñ–∑–æ–≤–∞–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç—É –≤ Markdown."""
        lines = []
        
        if include_metadata:
            lines.extend([
                "# –ê–Ω–æ–Ω—ñ–º—ñ–∑–æ–≤–∞–Ω–∏–π –¥–æ–∫—É–º–µ–Ω—Ç",
                "",
                "```",
                FileExporter._generate_metadata_header(result),
                "```",
                "",
                "---",
                ""
            ])
        
        lines.append(result.anonymized_text)
        
        content = "\n".join(lines)
        return content.encode('utf-8')
    
    @staticmethod
    def _export_full_report_md(result: AnalysisResult) -> bytes:
        """–ü–æ–≤–Ω–∏–π –∑–≤—ñ—Ç —É Markdown."""
        lines = [
            "# –ó–≤—ñ—Ç –ø—Ä–æ –¥–µ—ñ–¥–µ–Ω—Ç–∏—Ñ—ñ–∫–∞—Ü—ñ—é",
            "",
            "## –ú–µ—Ç–∞–¥–∞–Ω—ñ –∞–Ω–∞–ª—ñ–∑—É",
            "",
            "```",
            FileExporter._generate_metadata_header(result),
            "```",
            "",
            "## –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞",
            ""
        ]
        
        # Statistics table
        stats = FileExporter._calculate_statistics(result)
        lines.append("| –ü–æ–∫–∞–∑–Ω–∏–∫ | –ó–Ω–∞—á–µ–Ω–Ω—è |")
        lines.append("|----------|----------|")
        for key, value in stats.items():
            lines.append(f"| {key} | {value} |")
        
        lines.extend([
            "",
            "---",
            "",
            "## –ê–Ω–æ–Ω—ñ–º—ñ–∑–æ–≤–∞–Ω–∏–π —Ç–µ–∫—Å—Ç",
            "",
            result.anonymized_text,
            "",
            "---",
            "",
            "## –í–∏—è–≤–ª–µ–Ω—ñ —Å—É—Ç–Ω–æ—Å—Ç—ñ",
            ""
        ])
        
        if result.entities_count == 0:
            lines.append("‚úì –ü–µ—Ä—Å–æ–Ω–∞–ª—å–Ω–∏—Ö –¥–∞–Ω–∏—Ö –Ω–µ –≤–∏—è–≤–ª–µ–Ω–æ")
        else:
            # –ì—Ä—É–ø—É—î–º–æ –∑–∞ —Ç–∏–ø–∞–º–∏
            entities_by_type = {}
            for entity in result.entities:
                if entity.entity_type not in entities_by_type:
                    entities_by_type[entity.entity_type] = []
                entities_by_type[entity.entity_type].append(entity)
            
            for entity_type, entities in sorted(entities_by_type.items()):
                lines.append(f"### {entity_type} ({len(entities)} –∑–Ω–∞–π–¥–µ–Ω–æ)")
                lines.append("")
                
                for idx, entity in enumerate(sorted(entities, key=lambda x: x.start), 1):
                    entity_text = result.original_text[entity.start:entity.end]
                    lines.append(
                        f"{idx}. **'{entity_text}'** "
                        f"[–ø–æ–∑–∏—Ü—ñ—è {entity.start}-{entity.end}, "
                        f"–≤–ø–µ–≤–Ω–µ–Ω—ñ—Å—Ç—å {entity.score:.0%}]"
                    )
                
                lines.append("")
        
        content = "\n".join(lines)
        return content.encode('utf-8')
    
    # ============ HELPER METHODS ============
    
    @staticmethod
    def _generate_metadata_header(result: AnalysisResult) -> str:
        """–ì–µ–Ω–µ—Ä—É—î header –∑ –º–µ—Ç–∞–¥–∞–Ω–∏–º–∏."""
        return (
            f"–î–∞—Ç–∞ –æ–±—Ä–æ–±–∫–∏: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n"
            f"–ó–Ω–∞–π–¥–µ–Ω–æ —Å—É—Ç–Ω–æ—Å—Ç–µ–π: {result.entities_count}\n"
            f"–î–æ–≤–∂–∏–Ω–∞ –æ—Ä–∏–≥—ñ–Ω–∞–ª—å–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç—É: {len(result.original_text)} —Å–∏–º–≤–æ–ª—ñ–≤\n"
            f"–î–æ–≤–∂–∏–Ω–∞ –∞–Ω–æ–Ω—ñ–º—ñ–∑–æ–≤–∞–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç—É: {len(result.anonymized_text)} —Å–∏–º–≤–æ–ª—ñ–≤"
        )
    
    @staticmethod
    def _calculate_statistics(result: AnalysisResult) -> dict:
        """–û–±—á–∏—Å–ª—é—î —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –¥–ª—è –∑–≤—ñ—Ç—É."""
        # –ì—Ä—É–ø—É—î–º–æ –∑–∞ —Ç–∏–ø–∞–º–∏
        entities_by_type = {}
        for entity in result.entities:
            if entity.entity_type not in entities_by_type:
                entities_by_type[entity.entity_type] = []
            entities_by_type[entity.entity_type].append(entity)
        
        # –°–µ—Ä–µ–¥–Ω—è –≤–ø–µ–≤–Ω–µ–Ω—ñ—Å—Ç—å
        avg_confidence = (
            sum(e.score for e in result.entities) / len(result.entities)
            if result.entities else 0
        )
        
        stats = {
            "–ó–∞–≥–∞–ª—å–Ω–∞ –∫—ñ–ª—å–∫—ñ—Å—Ç—å —Å—É—Ç–Ω–æ—Å—Ç–µ–π": result.entities_count,
            "–£–Ω—ñ–∫–∞–ª—å–Ω–∏—Ö —Ç–∏–ø—ñ–≤ —Å—É—Ç–Ω–æ—Å—Ç–µ–π": len(entities_by_type),
            "–°–µ—Ä–µ–¥–Ω—è –≤–ø–µ–≤–Ω–µ–Ω—ñ—Å—Ç—å": f"{avg_confidence:.1%}",
            "–î–æ–≤–∂–∏–Ω–∞ –æ—Ä–∏–≥—ñ–Ω–∞–ª—å–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç—É": f"{len(result.original_text)} —Å–∏–º–≤–æ–ª—ñ–≤",
            "–î–æ–≤–∂–∏–Ω–∞ –∞–Ω–æ–Ω—ñ–º—ñ–∑–æ–≤–∞–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç—É": f"{len(result.anonymized_text)} —Å–∏–º–≤–æ–ª—ñ–≤"
        }
        
        # –î–æ–¥–∞—î–º–æ —Ä–æ–∑–ø–æ–¥—ñ–ª –ø–æ —Ç–∏–ø–∞—Ö
        for entity_type, entities in sorted(entities_by_type.items()):
            stats[f"  - {entity_type}"] = len(entities)
        
        return stats


# ============ CONVENIENCE FUNCTIONS ============

def generate_filename(
    base_name: str = "deidentified",
    format: str = ExportFormat.TXT,
    include_timestamp: bool = True
) -> str:
    """
    –ì–µ–Ω–µ—Ä—É—î —ñ–º'—è —Ñ–∞–π–ª—É –¥–ª—è –µ–∫—Å–ø–æ—Ä—Ç—É.
    
    Args:
        base_name: –ë–∞–∑–æ–≤–∞ –Ω–∞–∑–≤–∞ —Ñ–∞–π–ª—É
        format: –§–æ—Ä–º–∞—Ç –µ–∫—Å–ø–æ—Ä—Ç—É
        include_timestamp: –ß–∏ –¥–æ–¥–∞–≤–∞—Ç–∏ timestamp
        
    Returns:
        –ó–≥–µ–Ω–µ—Ä–æ–≤–∞–Ω–µ —ñ–º'—è —Ñ–∞–π–ª—É
    """
    if include_timestamp:
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        return f"{base_name}_{timestamp}.{format}"
    else:
        return f"{base_name}.{format}"